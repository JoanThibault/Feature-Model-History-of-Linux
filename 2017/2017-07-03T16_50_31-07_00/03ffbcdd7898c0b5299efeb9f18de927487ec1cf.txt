commit 03ffbcdd7898c0b5299efeb9f18de927487ec1cf
Merge: 1b044f1cfc65 f9632de40ee0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 3 16:50:31 2017 -0700

    Merge branch 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull irq updates from Thomas Gleixner:
     "The irq department delivers:
    
       - Expand the generic infrastructure handling the irq migration on CPU
         hotplug and convert X86 over to it. (Thomas Gleixner)
    
         Aside of consolidating code this is a preparatory change for:
    
       - Finalizing the affinity management for multi-queue devices. The
         main change here is to shut down interrupts which are affine to a
         outgoing CPU and reenabling them when the CPU comes online again.
         That avoids moving interrupts pointlessly around and breaking and
         reestablishing affinities for no value. (Christoph Hellwig)
    
         Note: This contains also the BLOCK-MQ and NVME changes which depend
         on the rework of the irq core infrastructure. Jens acked them and
         agreed that they should go with the irq changes.
    
       - Consolidation of irq domain code (Marc Zyngier)
    
       - State tracking consolidation in the core code (Jeffy Chen)
    
       - Add debug infrastructure for hierarchical irq domains (Thomas
         Gleixner)
    
       - Infrastructure enhancement for managing generic interrupt chips via
         devmem (Bartosz Golaszewski)
    
       - Constification work all over the place (Tobias Klauser)
    
       - Two new interrupt controller drivers for MVEBU (Thomas Petazzoni)
    
       - The usual set of fixes, updates and enhancements all over the
         place"
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (112 commits)
      irqchip/or1k-pic: Fix interrupt acknowledgement
      irqchip/irq-mvebu-gicp: Allocate enough memory for spi_bitmap
      irqchip/gic-v3: Fix out-of-bound access in gic_set_affinity
      nvme: Allocate queues for all possible CPUs
      blk-mq: Create hctx for each present CPU
      blk-mq: Include all present CPUs in the default queue mapping
      genirq: Avoid unnecessary low level irq function calls
      genirq: Set irq masked state when initializing irq_desc
      genirq/timings: Add infrastructure for estimating the next interrupt arrival time
      genirq/timings: Add infrastructure to track the interrupt timings
      genirq/debugfs: Remove pointless NULL pointer check
      irqchip/gic-v3-its: Don't assume GICv3 hardware supports 16bit INTID
      irqchip/gic-v3-its: Add ACPI NUMA node mapping
      irqchip/gic-v3-its-platform-msi: Make of_device_ids const
      irqchip/gic-v3-its: Make of_device_ids const
      irqchip/irq-mvebu-icu: Add new driver for Marvell ICU
      irqchip/irq-mvebu-gicp: Add new driver for Marvell GICP
      dt-bindings/interrupt-controller: Add DT binding for the Marvell ICU
      genirq/irqdomain: Remove auto-recursive hierarchy support
      irqchip/MSI: Use irq_domain_update_bus_token instead of an open coded access
      ...

diff --cc block/blk-mq-cpumap.c
index 2cca4fc43f45,5eaecd40f701..4891f042a22f
--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@@ -14,15 -14,10 +14,14 @@@
  #include "blk.h"
  #include "blk-mq.h"
  
- static int cpu_to_queue_index(unsigned int nr_queues, const int cpu,
- 			      const struct cpumask *online_mask)
 -static int cpu_to_queue_index(unsigned int nr_cpus, unsigned int nr_queues,
 -			      const int cpu)
++static int cpu_to_queue_index(unsigned int nr_queues, const int cpu)
  {
 -	return cpu * nr_queues / nr_cpus;
 +	/*
 +	 * Non online CPU will be mapped to queue index 0.
 +	 */
- 	if (!cpumask_test_cpu(cpu, online_mask))
++	if (!cpu_online(cpu))
 +		return 0;
 +	return cpu % nr_queues;
  }
  
  static int get_first_sibling(unsigned int cpu)
@@@ -40,27 -35,55 +39,26 @@@ int blk_mq_map_queues(struct blk_mq_tag
  {
  	unsigned int *map = set->mq_map;
  	unsigned int nr_queues = set->nr_hw_queues;
- 	const struct cpumask *online_mask = cpu_online_mask;
 -	unsigned int i, nr_cpus, nr_uniq_cpus, queue, first_sibling;
 -	cpumask_var_t cpus;
 -
 -	if (!alloc_cpumask_var(&cpus, GFP_ATOMIC))
 -		return -ENOMEM;
 -
 -	cpumask_clear(cpus);
 -	nr_cpus = nr_uniq_cpus = 0;
 -	for_each_present_cpu(i) {
 -		nr_cpus++;
 -		first_sibling = get_first_sibling(i);
 -		if (!cpumask_test_cpu(first_sibling, cpus))
 -			nr_uniq_cpus++;
 -		cpumask_set_cpu(i, cpus);
 -	}
 -
 -	queue = 0;
 -	for_each_possible_cpu(i) {
 -		if (!cpumask_test_cpu(i, cpu_present_mask)) {
 -			map[i] = 0;
 -			continue;
 -		}
 +	unsigned int cpu, first_sibling;
  
 +	for_each_possible_cpu(cpu) {
  		/*
 -		 * Easy case - we have equal or more hardware queues. Or
 -		 * there are no thread siblings to take into account. Do
 -		 * 1:1 if enough, or sequential mapping if less.
 +		 * First do sequential mapping between CPUs and queues.
 +		 * In case we still have CPUs to map, and we have some number of
 +		 * threads per cores then map sibling threads to the same queue for
 +		 * performace optimizations.
  		 */
 -		if (nr_queues >= nr_cpus || nr_cpus == nr_uniq_cpus) {
 -			map[i] = cpu_to_queue_index(nr_cpus, nr_queues, queue);
 -			queue++;
 -			continue;
 +		if (cpu < nr_queues) {
- 			map[cpu] = cpu_to_queue_index(nr_queues, cpu, online_mask);
++			map[cpu] = cpu_to_queue_index(nr_queues, cpu);
 +		} else {
 +			first_sibling = get_first_sibling(cpu);
 +			if (first_sibling == cpu)
- 				map[cpu] = cpu_to_queue_index(nr_queues, cpu, online_mask);
++				map[cpu] = cpu_to_queue_index(nr_queues, cpu);
 +			else
 +				map[cpu] = map[first_sibling];
  		}
 -
 -		/*
 -		 * Less then nr_cpus queues, and we have some number of
 -		 * threads per cores. Map sibling threads to the same
 -		 * queue.
 -		 */
 -		first_sibling = get_first_sibling(i);
 -		if (first_sibling == i) {
 -			map[i] = cpu_to_queue_index(nr_uniq_cpus, nr_queues,
 -							queue);
 -			queue++;
 -		} else
 -			map[i] = map[first_sibling];
  	}
  
 -	free_cpumask_var(cpus);
  	return 0;
  }
  EXPORT_SYMBOL_GPL(blk_mq_map_queues);
diff --cc block/blk-mq.c
index ced2b000ca02,dd390e27824d..6cef42f419a5
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -37,11 -37,9 +37,8 @@@
  #include "blk-wbt.h"
  #include "blk-mq-sched.h"
  
- static DEFINE_MUTEX(all_q_mutex);
- static LIST_HEAD(all_q_list);
- 
  static void blk_mq_poll_stats_start(struct request_queue *q);
  static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb);
 -static void __blk_mq_stop_hw_queues(struct request_queue *q, bool sync);
  
  static int blk_mq_poll_stats_bkt(const struct request *rq)
  {
diff --cc drivers/nvme/host/pci.c
index 33c3b9db7d36,b3dcd7abc6d7..5b1ac79fb607
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@@ -1692,10 -1523,9 +1692,10 @@@ static int nvme_setup_io_queues(struct 
  {
  	struct nvme_queue *adminq = dev->queues[0];
  	struct pci_dev *pdev = to_pci_dev(dev->dev);
 -	int result, nr_io_queues, size;
 +	int result, nr_io_queues;
 +	unsigned long size;
  
- 	nr_io_queues = num_online_cpus();
+ 	nr_io_queues = num_present_cpus();
  	result = nvme_set_queue_count(&dev->ctrl, &nr_io_queues);
  	if (result < 0)
  		return result;
